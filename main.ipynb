{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sottodirectory o file -p giï¿½ esistente.\n",
      "Errore occorso durante l'elaborazione: -p.\n",
      "ffmpeg version 7.1-essentials_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[in#0 @ 00000209e36d6980] Error opening input: No such file or directory\n",
      "Error opening input file IMG_7145.MOV.\n",
      "Error opening input files: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p frames_1\n",
    "!ffmpeg -i IMG_7145.MOV -f image2 -q:v 2 -vf \"fps=0.2\" frames_1/%05d.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.RandomResizedCrop(224, scale=(0.8, 1.0)),  # mild crop\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225])  # typical ImageNet normalization\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define the directory containing the images\n",
    "image_dir = r'C:\\Users\\barbo\\Few Shot VPR\\few-shot-vpr\\frames_1'\n",
    "\n",
    "# Create directories for train and test sets with a \"frames\" subfolder\n",
    "train_dir = os.path.join(image_dir, 'train', 'frames')  # Add \"frames\" subfolder\n",
    "test_dir = os.path.join(image_dir, 'test', 'frames')    # Add \"frames\" subfolder\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Get list of all image files\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move the files to the respective \"frames\" subfolders\n",
    "for file in train_files:\n",
    "    shutil.move(os.path.join(image_dir, file), os.path.join(train_dir, file))\n",
    "\n",
    "for file in test_files:\n",
    "    shutil.move(os.path.join(image_dir, file), os.path.join(test_dir, file))\n",
    "\n",
    "# Create ImageFolder datasets\n",
    "train_dataset = ImageFolder(root=os.path.join(image_dir, 'train'), transform=train_transforms)\n",
    "test_dataset = ImageFolder(root=os.path.join(image_dir, 'test'), transform=val_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ImageFolder(root='frames_1/train', transform=train_transforms)\n",
    "test_dataset   = ImageFolder(root='frames_1/test',   transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,   batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\barbo\\Few Shot VPR\\few-shot-vpr\\cnnimageretrieval-pytorch-master\\cnnimageretrieval-pytorch-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading checkpoint from retrievalSfM120k-resnet101-gem-b80fb85.pth\n",
      ">> Model architecture: resnet101\n",
      ">> Pooling type: gem\n",
      ">> imageretrievalnet.py: for 'resnet101' custom pretrained features 'imagenet-caffe-resnet101-features-10a101d.pth' are used\n",
      ">> Model loaded and moved to device: cpu\n",
      ">> Output dimension: 2048\n",
      ">> Descriptor shape: torch.Size([2048])\n",
      ">> First 5 values: tensor([4.4055e-02, 1.6614e-08, 1.6614e-08, 1.6614e-08, 1.5044e-02])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from cirtorch.networks.imageretrievalnet import init_network\n",
    "\n",
    "def main():\n",
    "    # ---------------------------------------------------------\n",
    "    # 1) Load the pretrained CIRTorch model checkpoint\n",
    "    # ---------------------------------------------------------\n",
    "    checkpoint_path = 'retrievalSfM120k-resnet101-gem-b80fb85.pth'  # your .pth file\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\">> Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # The checkpoint has 'state_dict' (weights) and 'meta' (architecture info)\n",
    "    net_params = checkpoint['meta']  \n",
    "    net_state_dict = checkpoint['state_dict']\n",
    "\n",
    "    print(f\">> Model architecture: {net_params['architecture']}\")\n",
    "    print(f\">> Pooling type: {net_params['pooling']}\")\n",
    "\n",
    "    # Initialize the network\n",
    "    model = init_network(net_params)     # creates ImageRetrievalNet\n",
    "    model.load_state_dict(net_state_dict)\n",
    "    model.eval()  # for inference (switch to train() if you plan to fine-tune)\n",
    "    model.to(device)\n",
    "\n",
    "    print(\">> Model loaded and moved to device:\", device)\n",
    "    print(\">> Output dimension:\", model.meta['outputdim'])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2) PARTIAL FREEZING (Light Fine-Tuning Setup)\n",
    "    # ---------------------------------------------------------\n",
    "    # A) Freeze all parameters first\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # B) Unfreeze the last ResNet block (layer4) IF it's a ResNet\n",
    "    if net_params['architecture'].startswith('resnet'):\n",
    "        # model.features is typically a Sequential of [conv1, bn1, relu, maxpool, layer1, layer2, layer3, layer4]\n",
    "        # The last element (index -1) is layer4 in ResNet101\n",
    "        for param in model.features[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # C) Unfreeze the GeM (or AP-GeM) pooling layer\n",
    "    for param in model.pool.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # D) If local/global whitening layers exist, unfreeze them too\n",
    "    if model.lwhiten is not None:\n",
    "        for param in model.lwhiten.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if model.whiten is not None:\n",
    "        for param in model.whiten.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Note: If you plan to do classification, you'd add a small classification head here.\n",
    "    # E.g. model.classifier = nn.Linear(model.meta['outputdim'], num_classes)\n",
    "\n",
    "    # If you were going to train, you'd now define an optimizer that only updates\n",
    "    # the parameters with requires_grad=True. For example:\n",
    "    #\n",
    "    # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    # model.train()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3) Define a transform matching model expectations\n",
    "    # ---------------------------------------------------------\n",
    "    custom_mean = model.meta['mean']\n",
    "    custom_std  = model.meta['std']\n",
    "\n",
    "    test_transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=custom_mean, std=custom_std),\n",
    "    ])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4) (Optional) Example: extract a descriptor from 1 image\n",
    "    # ---------------------------------------------------------\n",
    "    img_path = 'frames_1/test/frames/00006.jpg'\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = test_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass to extract descriptor\n",
    "    model.eval()  # ensure eval mode for extraction\n",
    "    with torch.no_grad():\n",
    "        descriptor = model(img_tensor)  # shape: [DIM, BATCH]\n",
    "    descriptor = descriptor.squeeze(1)  # shape [DIM]\n",
    "\n",
    "    print(\">> Descriptor shape:\", descriptor.shape)\n",
    "    print(\">> First 5 values:\", descriptor[:5])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading checkpoint from retrievalSfM120k-resnet101-gem-b80fb85.pth\n",
      ">> Architecture: resnet101\n",
      ">> Pooling: gem\n",
      ">> imageretrievalnet.py: for 'resnet101' custom pretrained features 'imagenet-caffe-resnet101-features-10a101d.pth' are used\n",
      ">> Backbone output dimension: 2048\n",
      "Epoch [1/5], Train Loss: 1.5632, Train Acc: 0.7963, Val Loss: 1.5010, Val Acc: 1.0000\n",
      "Epoch [2/5], Train Loss: 1.4812, Train Acc: 1.0000, Val Loss: 1.4007, Val Acc: 1.0000\n",
      "Epoch [3/5], Train Loss: 1.4030, Train Acc: 1.0000, Val Loss: 1.3198, Val Acc: 1.0000\n",
      "Epoch [4/5], Train Loss: 1.3364, Train Acc: 1.0000, Val Loss: 1.2706, Val Acc: 1.0000\n",
      "Epoch [5/5], Train Loss: 1.2709, Train Acc: 1.0000, Val Loss: 1.2363, Val Acc: 1.0000\n",
      ">> Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# CIRTorch import\n",
    "from cirtorch.networks.imageretrievalnet import init_network\n",
    "\n",
    "###############################\n",
    "# 1) Define a Classifier Wrapper\n",
    "###############################\n",
    "class VPRClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a CIRTorch backbone (which outputs [DIM, BATCH]) with a linear layer \n",
    "    for classification (output shape [BATCH, NUM_CLASSES]).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, embedding_dim=2048, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes) # maybe add 1 more layer in tuning\n",
    "    def forward(self, x):\n",
    "        # CIRTorch backbone returns shape [embedding_dim, batch]\n",
    "        feats = self.backbone(x)  # shape: [DIM, BATCH]\n",
    "        feats = feats.permute(1, 0)  # reshape to [BATCH, DIM] for classification\n",
    "        logits = self.classifier(feats)  # [BATCH, num_classes]\n",
    "        return logits\n",
    "\n",
    "###############################\n",
    "# 2) Main Training Script\n",
    "###############################\n",
    "# ---------------------------------------------------------\n",
    "# A) Load the Pretrained CIRTorch Model\n",
    "# ---------------------------------------------------------\n",
    "checkpoint_path = \"retrievalSfM120k-resnet101-gem-b80fb85.pth\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\">> Loading checkpoint from {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "# The checkpoint has 'state_dict' & 'meta'\n",
    "net_params = checkpoint['meta']       # architecture, pooling, etc.\n",
    "net_state_dict = checkpoint['state_dict']\n",
    "\n",
    "print(\">> Architecture:\", net_params['architecture'])\n",
    "print(\">> Pooling:\", net_params['pooling'])\n",
    "\n",
    "# Init the backbone (CIRTorch ImageRetrievalNet)\n",
    "backbone = init_network(net_params)\n",
    "backbone.load_state_dict(net_state_dict)\n",
    "backbone.eval()  # switch to inference mode for now\n",
    "backbone.to(device)\n",
    "\n",
    "print(\">> Backbone output dimension:\", backbone.meta['outputdim'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# B) Partial Freezing (Optional Light Fine-Tuning Setup)\n",
    "# ---------------------------------------------------------\n",
    "# 1) Freeze all\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Unfreeze only last ResNet block if architecture is ResNet\n",
    "if net_params['architecture'].startswith('resnet'):\n",
    "    # The last module in backbone.features is layer4 for resnet101\n",
    "    for param in backbone.features[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 3) Unfreeze the pooling layer\n",
    "for param in backbone.pool.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 4) (Optional) Unfreeze local/global whitening layers\n",
    "if backbone.lwhiten is not None:\n",
    "    for param in backbone.lwhiten.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "if backbone.whiten is not None:\n",
    "    for param in backbone.whiten.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# C) Create a VPRClassifier (Backbone + Linear Head)\n",
    "# ---------------------------------------------------------\n",
    "embedding_dim = backbone.meta['outputdim']  # typically 2048 for ResNet101\n",
    "num_classes = 5  # e.g., 5 buildings\n",
    "model = VPRClassifier(backbone, embedding_dim, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# D) Define Dataset, Transforms, and Dataloaders\n",
    "# ---------------------------------------------------------\n",
    "# Example using standard ImageFolder structure:\n",
    "#   data/train/class_0/*.jpg\n",
    "#   data/train/class_1/*.jpg\n",
    "#   ...\n",
    "#   data/val/class_0/*.jpg\n",
    "#   data/val/class_1/*.jpg\n",
    "# etc.\n",
    "\n",
    "# Use the mean/std from the backbone metadata\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=backbone.meta['mean'], std=backbone.meta['std']),\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=backbone.meta['mean'], std=backbone.meta['std']),\n",
    "])\n",
    "\n",
    "# Replace 'path_to_train' and 'path_to_val' with your actual folders\n",
    "train_dataset = ImageFolder(root='frames_1/train', transform=train_transforms)\n",
    "test_dataset   = ImageFolder(root='frames_1/test',   transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(test_dataset,   batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# E) Define Optimizer, Loss, and Move Model to Train Mode\n",
    "# ---------------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss() # Contrastive loss here\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4, \n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "model.train()  # enable train mode for dropout, BN, etc.\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# F) Training Loop\n",
    "# ---------------------------------------------------------\n",
    "num_epochs = 5  # or whatever you like (add early stopping)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(images)  # [batch_size, num_classes]\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate stats\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            val_logits = model(val_images)\n",
    "            v_loss = criterion(val_logits, val_labels)\n",
    "\n",
    "            val_loss += v_loss.item() * val_images.size(0)\n",
    "            val_preds = torch.argmax(val_logits, dim=1)\n",
    "            val_correct += torch.sum(val_preds == val_labels).item()\n",
    "            val_total += val_labels.size(0)\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\">> Training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class ID: 0\n",
      "Predicted class name: buildingA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 'buildingA')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from cirtorch.networks.imageretrievalnet import init_network\n",
    "\n",
    "###########################\n",
    "# Suppose you trained like this:\n",
    "###########################\n",
    "\n",
    "# 1) We assume 'model' is already trained and in memory, or re-load it:\n",
    "# model = VPRClassifier(...)\n",
    "# model.load_state_dict(...)\n",
    "# model.eval()\n",
    "\n",
    "# or you can just define a placeholder if you want to test structure:\n",
    "# model = ...\n",
    "# device = ...\n",
    "\n",
    "# 2) Define your transform (same as in training)\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    # use the same normalization as training\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 3) Suppose you have 5 classes\n",
    "class_names = ['buildingA', 'buildingB', 'buildingC', 'buildingD', 'buildingE']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "# test_single_image function\n",
    "###########################\n",
    "def test_single_image(model, img_path, transform, device, class_names):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "    pred_class_idx = preds.item()\n",
    "    pred_class_name = class_names[pred_class_idx]\n",
    "\n",
    "    print(\"Predicted class ID:\", pred_class_idx)\n",
    "    print(\"Predicted class name:\", pred_class_name)\n",
    "\n",
    "    return pred_class_idx, pred_class_name\n",
    "\n",
    "###########################\n",
    "# Now do the test\n",
    "###########################\n",
    "img_path = 'frames_1/test/frames/00006.jpg'\n",
    "test_single_image(model, img_path, transform, device, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_video(video_path, frame_rate, output_folder='dataset'):\n",
    "    \"\"\"\n",
    "    Processes a video, extracts frames at a specified frame rate, and saves them to a subdirectory.\n",
    "    Args:\n",
    "        video_path: Path to the input video file.\n",
    "        frame_rate: Desired frame rate for extraction.\n",
    "        output_folder: The name of the folder to store the frames\n",
    "    \"\"\"\n",
    "\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    subfolder_name = os.path.join(output_folder, video_name)  \n",
    "    os.makedirs(subfolder_name, exist_ok=True)\n",
    "\n",
    "    # Remove the \"!\" from the command so it runs correctly with os.system\n",
    "    ffmpeg_command = f\"ffmpeg -i '{video_path}' -f image2 -q:v 2 -vf 'fps={frame_rate}' '{subfolder_name}/%05d.jpg'\"\n",
    "    print(f\"Executing: {ffmpeg_command}\")\n",
    "    os.system(ffmpeg_command)\n",
    "\n",
    "# Example usage (in Colab):\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "os.chdir('/content/drive/MyDrive/CV videos')\n",
    "\n",
    "process_video('IMG_7145.MOV', 0.2)\n",
    "!ls -l dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
